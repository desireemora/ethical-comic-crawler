# Ethical Comic Crawler
Using `robotexclusionrulesparser` ensures that I am following all the rules defined by the robots.txt file of any website being scraped.

`pip install -r requirements.txt`

### Run
`python scrape.py`

### Run Tests
`python -m unittest test_issue.py`
