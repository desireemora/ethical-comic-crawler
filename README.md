# Ethical Comic Crawler
Using `robotexclusionrulesparser` ensures that I am following all the rules defined by the robots.txt file of any website being scraped.

`pip install -r requirements.txt`
